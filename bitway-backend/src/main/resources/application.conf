include "serialization"
akka {
    log-dead-letters = 1
    log-dead-letters-during-shutdown = false
    event-handlers = ["akka.event.slf4j.Slf4jEventHandler"]
    loglevel = DEBUG
    loggers = ["akka.event.slf4j.Slf4jLogger"]

    extensions = [
        "akka.contrib.pattern.DistributedPubSubExtension",
        "akka.contrib.pattern.ClusterReceptionistExtension"
    ]

    cluster-dispatcher {
        type = "Dispatcher"
        executor = "fork-join-executor"
        throughput = 50
        fork-join-executor {
            parallelism-min = 2
            parallelism-factor = 2.0
            parallelism-max = 16
        }
    }

    actor {
        provider = "akka.cluster.ClusterActorRefProvider"
        serialize-messages = off # this will be off in production
    }

    remote {
        enabled-transports = ["akka.remote.netty.tcp"]
        netty.tcp {
           maximum-frame-size = 512000b
           receive-buffer-size = 1024000b
           send-buffer-size = 1024000b
        }
    }

    cluster {
        use-dispatcher = "akka.cluster-dispatcher"
        auto-down-unreachable-after = 10s
    }

    invoice {
        mongo-uri-for-readers = "mongodb://localhost:27017/bitway_readers"
    }

    mailer {
        mandrill-api-key="VsInYd0SdL16YM1JlfVdcQ"
        invoice-url-base="http://pay.coinport.com/api/v1/invoice/"
    }

    blockchain {
        config-path = "blockchain.scala"
    }

    payment {
        transfer-path = "account_transfer-prod.scala"
    }

    persistence {
      view.auto-update-interval = 1000ms

      #journal.plugin = "akka-contrib-mongodb-persistence-journal"
      #journal.plugin = "hbase-journal"

      #snapshot-store.plugin = "akka-contrib-mongodb-persistence-snapshot"
      #snapshot-store.plugin = "hadoop-snapshot-store"

      # we need event publishing for tests
      # publish-confirmations = on
      publish-plugin-commands = off

      # disable leveldb (default store impl)
      journal.leveldb.native = on

      encryption-settings = "import akka.persistence.hbase.common.EncryptionConfig;new EncryptionConfig(keyMap = Map(1 -> \"j$2#5:?3ac@~dlcR\".getBytes))"

      replay-gap-retry = 5
      skip-gap = off

      export-sequence {
        enable-export = false
        processor-id = "p_m_btccny"
        #file = "/tmp/market_view.txt"
        file = "/tmp/market_processor.txt"
      }
    }

    trader {
        config-path = "trader.scala"
    }

    invoice {
        config-path = "invoice.scala"
    }
}

hbase-journal {
  table = "bw"
  family = "a"
  # has been hardcode to 18, this configure will be ignore
  # partition.count= 18
  scan-batch-size = 20
  client-flush-interval = 0
  publish-testing-events = off

  # For HBase sync
  plugin-dispatcher = "akka-hbase-persistence-dispatcher"

  # Original config
  replay-dispatcher = "default-replay-dispatcher"

  class = "akka.persistence.hbase.journal.HBaseAsyncWriteJournal"
  hbase {
    cluster.distributed = false
    zookeeper.quorum = "hadoop:2181"
  }
}

hadoop-snapshot-store {
  hdfs-default-name = "hdfs://hadoop:54310"
    snapshot-dir = "/snapshot_bw/"
    publish-testing-events = off
    class = "akka.persistence.hbase.snapshot.HadoopSnapshotStore"

    # For HBase sync
    plugin-dispatcher = "akka-hbase-persistence-dispatcher"

    # Original config
    replay-dispatcher = "default-replay-dispatcher"

    impl = "akka.persistence.hbase.snapshot.HdfsSnapshotter"

    # HBaseSnapshotter config
    #impl = "akka.persistence.hbase.snapshot.HBaseSnapshotter"
    #table = "snapshot_bw"
    #family = "a"
    #client-flush-interval = 0
    #hbase {
    #  cluster.distributed = false
    #  zookeeper.quorum = "hadoop:2181"
    #}
}

akka-hbase-persistence-dispatcher {
  type = Dispatcher
  executor = "thread-pool-executor"
  thread-pool-executor {
    core-pool-size-min = 2
    core-pool-size-factor = 2.0
    core-pool-size-max = 10
  }
  throughput = 100
}

default-replay-dispatcher {
  type = Dispatcher
  executor = "fork-join-executor"
  fork-join-executor {
    parallelism-min = 2
    parallelism-max = 8
  }
}

#?autoReconnect=true&useUnicode=true&characterEncoding=utf8
db.default.driver=com.mysql.jdbc.Driver
db.default.url="jdbc:mysql://localhost/coinport?autoReconnect=true"
db.default.user="coinport"
db.default.password="coinport4payment"
db.default.poolInitialSize=10
db.default.poolMaxSize=20
db.default.connectionTimeoutMillis=1000
db.default.validationQuery="Select 1"

scalikejdbc.play.closeAllOnStop.enabled=true
logger.scalikejdbc=DEBUG
